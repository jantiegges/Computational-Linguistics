\documentclass[10pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amssymb, graphicx, multicol, array, parskip}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{url}
% use straight quotes in texttt environment, make 0 different from O
\usepackage[zerostyle=b,straightquotes,scaled=.93]{newtxtt}
\usepackage{siunitx}

\usepackage{listings}
\lstset{
 basicstyle=\ttfamily,
 columns=fullflexible,
 upquote,
 keepspaces,
 literate={*}{{\char42}}1
 {-}{{\char45}}1
}
\usepackage[short labels]{enumitem}

\usepackage{soul}  % for strike through (\st{})
\usepackage[dvipsnames,usenames,table]{xcolor} % for colors
\usepackage[htt]{hyphenat} % to break lines in texttt

\usepackage[framemethod=TikZ]{mdframed}
\usepackage{mdframed}
\global\mdfdefinestyle{simple}{linewidth=1pt,skipabove=.5em}
\newenvironment{AnswerBox}{\begin{mdframed}[style=simple]}{\end{mdframed}}
\newcommand\defeq{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily def}}}{=}}}
\usepackage[colorlinks]{hyperref} % \href{http://link.com}{link text}
\hypersetup{linkcolor=NavyBlue,citecolor=NavyBlue,filecolor=NavyBlue,urlcolor=NavyBlue}
\usepackage{dsfont}
\newcommand{\required}[1]{{\color{blue}{#1}}}
\newcommand{\email}[1]{\href{mailto:#1}{\texttt{#1}}}
\newcommand{\PSnum}{3}

\author{
  \textbf{Name}: Jan Tiegges       %Put your name here.
, \textbf{McGill ID}: 261180937 %Put your McGill ID here.
\\ \textit{Collaborators}:  %Put the names of up to 2 people if you completed the assignment as a group.
}

\begin{document}

\title{LING/COMP 445, LING 645\\Problem Set \PSnum}
\date{Due before 4:35 PM on Wednesday, October 18, 2023}
\maketitle
Please enter your name and McGill ID above.
There are several types of questions below.
\begin{itemize}
\item
For questions involving answers in English or mathematics or a
combination of the two, put your answers to the question in an
answer box like in the example below. You can find more
information about \LaTeX{} here \url{https://www.latex-project.org/}.

\item For programming questions,
please put your answers into a file called
\texttt{ps\PSnum-lastname-firstname.clj}. Be careful to follow the instructions
exactly and be sure that all of your function definitions use the
precise names, number of inputs and input types, and output types as
requested in each question.

For the code portion of the assignment, \textbf{it is crucial to submit a
standalone file that runs}. Before you submit \texttt{ps\PSnum-lastname-firstname.clj},
make sure that your code executes correctly without any errors
when run at the command line by typing
\texttt{clojure ps\PSnum-lastname-firtname.clj} at a terminal
prompt. We cannot grade any code that does not run correctly as a
standalone file, and if the preceding command produces an error,
the code portion of the assignment will receive a $0$.

To do the computational problems, we recommend that you install
Clojure on your local machine and write and debug the answers to each
problem in a local copy of \texttt{ps\PSnum-lastname-firstname.clj}. You can
find information about installing and using Clojure here
\url{https://clojure.org/}.

A template Clojure file will be provided with the helper
functions described below.
\end{itemize}
Once you have entered your answers, please compile your copy of this
\LaTeX{} into a PDF and submit
\begin{enumerate}[(i),noitemsep]
\item
the compiled PDF renamed to
\texttt{ps\PSnum-lastname-firstname.pdf}
\item
the raw \LaTeX{} file renamed to
\texttt{ps\PSnum-lastname-firstname.tex} and
\item
your \texttt{ps\PSnum-lastname-firstname.clj}
\end{enumerate}
to the Problem Set \PSnum{} folder under `Assignments' on MyCourses.

\hrulefill %--------------------------------

\paragraph{Example Problem:}
This is an example question using some fake math like this
$L=\sum_0^{\infty} \mathcal{G} \delta_x$.

\paragraph{Example Answer:} Put your answer in the box provided, like this:
\begin{AnswerBox}
Example answer is $L=\sum_0^{\infty} \mathcal{G} \delta_x$.
\end{AnswerBox}


\hrulefill%--------------------------------

\pagebreak%
\section*{MAIN PROBLEM SET}
\paragraph{Problem 1:}

In these exercises, we are going to be processing some natural linguistic data,
the first paragraph of Moby Dick. We will first write some procedures that help
us to manipulate this corpus. We will then start analyzing this data using some
probabilistic models.

We'll start by defining the variable \texttt{moby-word-tokens}, the tokens in
our corpus as a Clojure list. This variable is defined for you already in
the provided template Clojure file.

\begin{lstlisting}
  (def moby-word-tokens '(CALL me Ishmael . Some years ago never mind how long
    precisely having little or no money in my purse , and nothing particular to
    interest me on shore , I thought I would sail about a little and see the
    watery part of the world . It is a way I have of driving off the spleen ,
    and regulating the circulation . Whenever I find myself growing grim about
    the mouth whenever it is a damp , drizzly November in my soul whenever I
    find myself involuntarily pausing before coffin warehouses , and bringing up
    the rear of every funeral I meet and especially whenever my hypos get such
    an upper hand of me , that it requires a strong moral principle to prevent
    me from deliberately stepping into the street , and methodically knocking
    people's hats off then , I account it high time to get to sea as soon as I
    can . This is my substitute for pistol and ball . With a philosophical
    flourish Cato throws himself upon his sword I quietly take to the ship .
    There is nothing surprising in this . If they but knew it , almost all men
    in their degree , some time or other , cherish very nearly the same feelings
    toward the ocean with me .))
\end{lstlisting}

In the template Clojure file, we have also defined the function
\texttt{member-of-list?}, with the following code.
\begin{lstlisting}
  (defn member-of-list? [w l]
    (if (empty? l)
      false
      (if (= w (first l))
        true
        (member-of-list? w (rest l)))))
\end{lstlisting}
It has two arguments, \texttt{w} and \texttt{l}, and returns \texttt{true} if
\texttt{w} is a member of the list \texttt{l}, and \texttt{false} otherwise. For
example, \texttt{(member-of-list? 'a '(a ship is))}, will return
\texttt{true}, and \texttt{(member-of-list? 'the '(a ship is))} will return
\texttt{false}.

The template Clojure file contains a skeleton for the function
\required{\texttt{get-vocabulary}}, which you must implement. This function
takes two arguments, \texttt{word-tokens} and \texttt{vocab}, and it should
return a list of all unique words occurring in \texttt{word-tokens}. For
example, if \texttt{word-tokens} is \texttt{'(the ship is the ship)}, then
get-vocabulary should return \texttt{'(the ship is)}. Implement this function by
filling in the missing parts of this provided code.

When you call \texttt{(get-vocabulary moby-word-tokens '())}, you will get back
a list of all of the unique words occurring in \texttt{moby-word-tokens}. Give
this the name \required{\texttt{moby-vocab}}.

% \begin{lstlisting}
% ;;(defn get-vocabulary [word-tokens vocab]
% ;;  (if (empty? word-tokens)
% ;;    vocab
% ;;    (if (member-of-list? ;;finish this line
% ;;      (get-vocabulary  ;;finish this line
% ;;      (get-vocabulary  ;;finish this line
% \end{lstlisting}

\textbf{Note}: there are a lot of choices that go into processing text when
doing work with a corpus. That is not the point of this problem set.  To make
things easier:
\begin{itemize}[nosep]
  \item Don't worry about case.   So, treat \texttt{With} and \texttt{with} as
    different words.
  \item Don't worry about punctuation.  Treat \texttt{.} as a word just like any
    other. Also, note that commas are treated as whitespace in Clojure so they
    will be ignored by your code. We left them in the list
    \texttt{moby-word-tokens} just for readability.
\end{itemize}


\paragraph{Answer 1:} Please put your answer in
\texttt{ps\PSnum-lastname-firstname.clj}.

\hrulefill %--------------------------------

\paragraph{Problem 2:}

Define a function \required{\texttt{get-count-of-word}}. This function should
take three arguments, \texttt{w}, \texttt{word-tokens}, and \texttt{count},
where \texttt{w} is a word, \texttt{word-tokens} is a list of words, and
\texttt{count} is a number.

% \begin{lstlisting}
% ;;(defn get-count-of-word [w word-tokens count]
%   ;;fill this in
% \end{lstlisting}

When you call \texttt{(get-count-of-word w word-tokens 0)},  the function should
return the number of occurrences of the word \texttt{w} in the list
\texttt{word-tokens}. For example
\begin{itemize}[noitemsep]
  \item \texttt{(get-count-of-word 'the (list 'the 'the 'whale) 0)} should
    return \texttt{2}.
  \item \texttt{(get-count-of-word 'the (list 'the 'whale) 0)} should return
    \texttt{1}.
\end{itemize}
Write \texttt{get-count-of-word} as a recursive function.   You can use the
\texttt{count} argument to accumulate the words counted so far.

\paragraph{Answer 2:} Please put the answer in
\texttt{ps\PSnum-lastname-firstname.clj}.

\hrulefill %--------------------------------

\paragraph{Problem 3:}

In the template Clojure file, we have provided a function
\texttt{get-word-counts}, which takes two arguments, \texttt{vocab} and
\texttt{word-tokens}, where \texttt{vocab} is assumed to be a list of the unique
words that occur in the list \texttt{word-tokens}.
\begin{lstlisting}
  (defn get-word-counts [vocab word-tokens]
    (let [count-word
          (fn [w] (get-count-of-word w word-tokens 0))]
      (map count-word vocab)))
\end{lstlisting}

This function returns the number of times each word in \texttt{vocab} occurs in
\texttt{word-tokens}. For example, suppose \texttt{vocab} is  \texttt{'(whale
the is)}, and \texttt{word-tokens} is  \texttt{'(the is whale is)}.  Then the
function will return the list \texttt{(1 1 2)}, corresponding to the number of
times \texttt{'whale}, \texttt{'the}, and \texttt{'is} occur in
\texttt{word-tokens}, respectively.

Use this function and the other variables we have
defined, to define a variable named \required{\texttt{moby-word-frequencies}}.
This variable should contain the number of times each word in
\texttt{moby-vocab} occurs in \texttt{moby-word-tokens}.

\paragraph{Answer 3:} Please put your answer in
\texttt{ps\PSnum-lastname-firstname.clj}.

\hrulefill %--------------------------------


\paragraph{Problem 4:}

In class we defined the functions \texttt{normalize}, \texttt{flip}, and
\texttt{sample-categorical}. These functions will be very useful for us, and are
included below as well as in the Clojure template file.

\begin{lstlisting}
  (defn flip [p]
    (if (< (rand 1) p)
      true
      false))

  (defn normalize [params]
    (let [sum (apply + params)]
      (map (fn [x] (/ x sum)) params)))

  (defn sample-categorical [outcomes params]
    (if (flip (first params))
      (first outcomes)
      (sample-categorical (rest outcomes) (normalize (rest params)))))
\end{lstlisting}

We have also provided a function that returns a particular probability
distribution, the \emph{uniform distribution}. The uniform distribution is the
distribution which assigns equal probability to every possible outcome. The
function \texttt{create-uniform-distribution} takes a single argument,
\texttt{outcomes}, which is a list of length $n$. The function returns a list
containing the number $1/n$ repeated $n$ times. For example, if
\texttt{outcomes} is \texttt{'(the a every)}, then this function will return
\texttt{'(1/3 1/3 1/3)}. This list can be interpreted as a probability
distribution over the outcomes, which assigns equal probability to each of them.

\begin{lstlisting}
(defn create-uniform-distribution [outcomes]
  (let [num-outcomes (count outcomes)]
    (map
      (fn [x] (/ 1 num-outcomes))
      outcomes)))
\end{lstlisting}

Using functions \texttt{create-uniform-distribution} and
\texttt{sample-categorical}, write a function
\required{\texttt{sample-uniform-BOW-sentence}} that takes two arguments: a
number \texttt{n} and a list \texttt{vocab}, and returns a  sentence of length
\texttt{n}. Each word in the sentence  should be generated independently from
the uniform distribution over vocab. For example, if \texttt{n} is \texttt{4}
and  \texttt{vocab} is \texttt{'(the a every)}, a possible return  value for
this function is \texttt{'(a the the a)}.

Note that this is a bag of words model, as defined in class. That is, we assume
every element of the list is generated independently. We will call this the
uniform bag of words model.

\paragraph{Answer 4:} Please put your answer in
\texttt{ps\PSnum-lastname-firstname.clj}.

\hrulefill %--------------------------------

\paragraph{Problem 5:}

Define a function \required{\texttt{compute-uniform-BOW-prob}},  which takes two
arguments, \texttt{vocab} and \texttt{sentence}.  \texttt{vocab} is the list of
all words in the vocabulary, and \texttt{sentence} is a list of observed words.
The function should return the probability of the sentence  according to the
uniform bag of words model.

For example, if \texttt{vocab} is \texttt{'(the a every)}, and sentence is
\texttt{'(every every)}, then the function should return the number
$\frac{1}{9}$.

\paragraph{Answer 5:} Please put your answer in
\texttt{ps\PSnum-lastname-firstname.clj}.

\hrulefill %--------------------------------

\paragraph{Problem 6:}

Using \texttt{sample-uniform-BOW-sentence} and \texttt{moby-vocab}, sample a
3-word sentence from the vocabulary of our Moby Dick corpus. This will be a
sample from the uniform bag of words model for this vocabulary. Repeat this
process a handful of times. For each of these 3-word sentences, use
\texttt{compute-uniform-BOW-prob} to compute the probability of the sentence
according to the uniform bag of words model. Are the different sentences you
sampled assigned different probabilities under this model? Explain why this is
(or isn't) to be expected.

\paragraph{Answer 6:} Please put your answer in the box below.

\begin{AnswerBox}%% Do not delete %%%%%%%

    %%%%%%%%%%%%%%%%%%%%%%%
    %% YOUR ANSWER HERE. %%
    %%%%%%%%%%%%%%%%%%%%%%%
    The sentence probability is always identical $\frac{1}{2744000}$. This is due to the fact, that the probability among the different words in the vocabulary is uniformly distributed, meaning that no matter which words are being sampled, the probability will be unaffected. Would we sample words from outside the vocabulary, we could achieve a probability of 0 as alternative, but as we sample from the vocabulary only, we will always achieve the same probability.

\end{AnswerBox}%% Do not delete %%%%%%%

\hrulefill %--------------------------------

\paragraph{Problem 7:}

In class we looked at a more general version of the bag of words
model, in which different words in the vocabulary can be assigned
different probabilities. We defined a function \texttt{sample-BOW-sentence},
which returns a sentence sampled from the bag of words model that we
have specified. Below we have included a slight variant of the
function which we defined in class. Previously the variables
vocabulary and probabilities were defined outside of the function. In
the current version, they are passed in as arguments. The function is
identical otherwise.

\begin{lstlisting}
(defn sample-BOW-sentence [len vocabulary probabilities]
  (if (= len 0)
    '()
    (cons (sample-categorical vocabulary probabilities)
	  (sample-BOW-sentence (- len 1) vocabulary probabilities))))
\end{lstlisting}

The function \texttt{sample-BOW-sentence} allows us to sample a
sentence given arbitrary probabilities for the words in our
vocabulary. Let's make use of this power and define a distribution
over the vocabulary which is better than the uniform distribution. We
will use the word frequencies for our Moby Dick corpus to
\emph{estimate} a better distribution.

Above we defined the variable \texttt{moby-word-frequencies}, which
contains the frequency of every word that occurs in our Moby Dick
corpus. Using \texttt{normalize} and \texttt{moby-word-frequencies},
define a variable \required{\texttt{moby-word-probabilities}}. This variable
should contain probabilities for every word in \texttt{moby-vocab}, in
proportion to its frequency in the text. A word which occurs 2 times
should receive twice as much probability as a word which occurs 1
time.

\paragraph{Answer 7:} Please put your answer in
\texttt{ps\PSnum-lastname-firstname.clj}.

\hrulefill %--------------------------------

\paragraph{Problem 8:}

Using \texttt{sample-BOW-sentence}, sample a 3-word sentence from a
bag of words model, in which the probabilities are set to be those in
\texttt{moby-word-probabilities}. Repeat this process at least three
times, and write down the sentences that you collect through this
process.

\paragraph{Answer 8:} Please put the output sentences in the box below.

\begin{AnswerBox}%% Do not delete %%%%%%%

    %%%%%%%%%%%%%%%%%%%%%%%
    %% YOUR ANSWER HERE. %%
    %%%%%%%%%%%%%%%%%%%%%%%
    the stepping I
    methodically precisely the
    hand it almost
\end{AnswerBox}%% Do not delete %%%%%%%

\hrulefill %--------------------------------

\paragraph{Problem 9:}

Define a function \required{\texttt{lookup-probability}}, which takes three
arguments, \texttt{w}, \texttt{outcomes}, and
\texttt{probs}. \texttt{probs} represents a probability distribution
over the elements of \texttt{outcomes}. For example, if outcomes is
\texttt{'(the a every)}, then \texttt{probs} may be
\texttt{'(0.2 0.5 0.3)}. The first number in \texttt{probs} is the
probability of the first element of outcomes, the second number in
probs is the probability of the second element of outcomes, and so on.

\texttt{lookup-probability} should look up the probability of the
element \texttt{w}. For example, if \texttt{w} is \texttt{'the},
then look-up probability should return \texttt{0.2}. If \texttt{w}
is \texttt{'a}, then \texttt{lookup-probability} should return
\texttt{0.5}.  If \texttt{w} is not in the list of outcomes,
the function should return that its probability is zero.

\paragraph{Answer 9:} Please put your answer in
\texttt{ps\PSnum-lastname-firstname.clj}.

\hrulefill %--------------------------------

\paragraph{Problem 10:}

Using \texttt{lookup-probability}, define a
function \required{\texttt{compute-BOW-prob}} which takes three arguments,
\texttt{sentence}, \texttt{vocabulary}, and
\texttt{probabilities}. The arguments \texttt{vocabulary} and
\texttt{probabilities} are used to define a bag of words model with
the associated probability distribution over vocabulary words. The
function should compute the probability of the sentence (which is a
list of words) according to the bag of words model.

This function is a generalization of the function
\texttt{compute-uniform-BOW-prob} that you defined above.

\paragraph{Answer 10:} Please put your answer in
\texttt{ps\PSnum-lastname-firstname.clj}.

\hrulefill %--------------------------------

\paragraph{Problem 11:}

In problem 8, you collected a number of 3-word sentences. These
sentences were generated from a bag of words model in which the
probabilities were set to those in \texttt{moby-word-probabilities},
which reflect the relative frequency of the words in the Moby Dick
corpus. Use \texttt{compute-BOW-prob} to compute the probability of
these sentences according to the bag of words model. How does your
answer differ from problem 6?

Choose one of the 3-word sentences that you have generated. Can you
construct a different sentence which has the same probability
according to the bag of words model? When computing the probability of
a sentence under a bag of words model, what information about the
sentence suffices to compute this probability?

\paragraph{Answer 11:} Please put your answer in the box below.

\begin{AnswerBox}%% Do not delete %%%%%%%

    %%%%%%%%%%%%%%%%%%%%%%%
    %% YOUR ANSWER HERE. %%
    %%%%%%%%%%%%%%%%%%%%%%%
    Sentence Probabilities: 
    \begin{enumerate}
      \item the stepping I: \num{9.858e-6}
      \item methodically precisely the: \num{1.095e-6}
      \item hand it almost: \num{4.381e-7}
      \item stepping it almost: \num{4.381e-7}
    \end{enumerate}
    We no longer use the uniform distribution but a probability distribution based on the frequency of the words in the corpus, meaning that the probabilities of the individual words can differ because they occur with different frequency. Therefore, we do not always get the same probability for our sentence as in the uniform case. However, some words still have the same frequency in the corpus and therefore the same probability, which is why we can generate different sentences with the same probability. This can be observed with the sentences \textit{hand it almost} and /textit{stepping it almost}, which both have the same probability, because the words \textit{hand} and \textit{stepping} occur equally often in the corpus (1 time). In general, the number of occurrences of each word in a sentence in the corpus is sufficient for calculating the probability in a bag of words model, since we can simply calculate 1 divided by the multiplication of these numbers.

\end{AnswerBox}%% Do not delete %%%%%%%

%--------------------------------
\newpage
\section*{LONG FORM READING QUESTION:} 
\textbf{(This section is optional for students in LING/COMP 445, but must be completed if taking LING 645.)}

You must answer this question on your own.

Elman's (1991) seminal work is an early example of how neural networks, here recurrent neural networks, can be used to model cognitive representations and learning processes. This work is part of a theoretical tradition called Connectionism which came from psychology and cognitive science and posited that all behaviours, especially linguistic, could be explained via simple neural connections, here examplified by neural network models. Elman discusses the distinctions between traditional computational models of language and neural network models. (1) Name and describe at least two of these distinctions. (2) Given that extracting information about linguistic representation from neural network models requires probing techniques, what is a limitation of this modeling approach? (Your answer should be 1/2 a page to a page in length.)


\paragraph{Answer:} Please put your answer in the box below.

\begin{AnswerBox}%% Do not delete %%%%%%%

  %%%%%%%%%%%%%%%%%%%%%%%
  %% YOUR ANSWER HERE. %%
  %%%%%%%%%%%%%%%%%%%%%%%
  Traditional computational language models are often based on explicit rules or algorithms used to generate or analyze sentences. In contrast, neural network models use connectionist learning, where the network learns to make predictions or produce outputs based on patterns in the input data. This learning is often implicit and distributed across the network, rather than being explicitly programmed. The authors claim that the latter is however not necessarily a no-rule system, since it is unclear what exactly is meant by a rule. They therefore argue that the difference lies mainly in the nature of the rules used and what kind of information counts as explicitly present. Furthermore, traditional language models rely on discrete and abstract symbols representing linguistic units such as words and phrases. Neural network models, on the other hand, create representations corresponding to the activation behaviours of the individual nodes. Such representations are not bound to specific linguistic units, but result from the learning process of the network. 
  
  Probing techniques involve testing the behavior of the network on specific tasks to draw conclusions about the nature of the learned linguistic representation or tasks. However, it can be difficult to design these tests to accurately capture the relevant linguistic phenomena, and it can be difficult to interpret the results of these tests. Moreover, it is not always clear how the network's behavior on these tests generalizes to its behavior on other tasks or in other contexts.

\end{AnswerBox}%% Do not delete %%%%%%%

\end{document}
