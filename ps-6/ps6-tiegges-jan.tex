\documentclass[10pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb, graphicx, multicol, array, parskip}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{url}
% use straight quotes in texttt environment, make 0 different from O
\usepackage[zerostyle=b,straightquotes,scaled=.93]{newtxtt}

\usepackage{listings}
\lstset{
 basicstyle=\ttfamily,
 columns=fullflexible,
 upquote,
 keepspaces,
 literate={*}{{\char42}}1
 {-}{{\char45}}1
}
\usepackage[short labels]{enumitem}

\usepackage{soul}  % for strike through (\st{})
\usepackage[dvipsnames,usenames,table]{xcolor} % for colors
\usepackage[htt]{hyphenat} % to break lines in texttt

\usepackage[framemethod=TikZ]{mdframed}
\usepackage{mdframed}
\global\mdfdefinestyle{simple}{linewidth=1pt,skipabove=.5em}
\newenvironment{AnswerBox}{\begin{mdframed}[style=simple]}{\end{mdframed}}
\newcommand\defeq{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily def}}}{=}}}
\usepackage[colorlinks]{hyperref} % \href{http://link.com}{link text}
\hypersetup{linkcolor=NavyBlue,citecolor=NavyBlue,filecolor=NavyBlue,urlcolor=NavyBlue}
\usepackage{dsfont}
\newcommand{\required}[1]{{\color{blue}{#1}}}
\newcommand{\email}[1]{\href{mailto:#1}{\texttt{#1}}}
\newcommand{\PSnum}{6}

\author{
  \textbf{Name}: Jan Tiegges       %Put your name here.
, \textbf{McGill ID}: 261180937 %Put your McGill ID here.
\\ \textit{Collaborators}:  %Put the names of up to 2 people if you completed the assignment as a group.
}

\begin{document}

\title{LING/COMP 445, LING 645\\Problem Set \PSnum}
\date{Due before 4:35 PM on Wednesday, December 13, 2023}
\maketitle
Please enter your name and McGill ID above.
There are several types of questions below. 
\begin{itemize}
\item
For questions involving answers in English or mathematics or a
combination of the two, put your answers to the question in an
\textbf{Answer} section like in the example below. You can find more
information about \LaTeX{} here \url{https://www.latex-project.org/}.

\item For programming questions,
please put your answers into a file called
\texttt{ps\PSnum-lastname-firstname.clj}. Be careful to follow the instructions
exactly and be sure that all of your function definitions use the
precise names, number of inputs and input types, and output types as
requested in each question.

For the code portion of the assignment, \textbf{it is crucial to submit a
standalone file that runs}. Before you submit \texttt{ps\PSnum-lastname-firstname.clj}, 
make sure that your code executes correctly without any errors 
when run at the command line by typing 
\texttt{clojure ps\PSnum-lastname-firtname.clj} at a terminal
prompt. We cannot grade any code that does not run correctly as a
standalone file, and if the preceding command produces an error,
the code portion of the assignment will receive a $0$.

To do the computational problems, we recommend that you install
Clojure on your local machine and write and debug the answers to each
problem in a local copy of \texttt{ps\PSnum-lastname-firstname.clj}. You can
find information about installing and using Clojure here
\url{https://clojure.org/}.
\end{itemize}
Once you have entered your answers, please compile your copy of this
\LaTeX{} into a PDF and submit 
\begin{enumerate}[(i),noitemsep]
\item
the compiled PDF renamed to
\texttt{ps\PSnum-lastname-firstname.pdf} 
\item
the raw \LaTeX{} file renamed to
\texttt{ps\PSnum-lastname-firstname.tex} and 
\item
your \texttt{ps\PSnum-lastname-firstname.clj}
\end{enumerate}
to the Problem Set \PSnum{} folder under `Assignments' on MyCourses.

\hrulefill %--------------------------------

\paragraph{Example Problem:}
This is an example question using some fake math like this
$L=\sum_0^{\infty} \mathcal{G} \delta_x$.

\paragraph{Example Answer:} Put your answer in the box provided, like this:
\begin{AnswerBox}
Example answer is $L=\sum_0^{\infty} \mathcal{G} \delta_x$.
\end{AnswerBox}

\pagebreak%

\hrulefill%--------------------------------
\paragraph{Background:}
Throughout this problem set we will be working with the Hidden Markov
Model. In an HMM, there is a sequence of hidden states
$c^{(1)},...,c^{(n)}$, and a sequence of observed words,
$w^{(1)},...,w^{(n)}$. The set of all possible hidden states is $S$,
and the set of all possible words is $O$. Thus $c^{(i)} \in S$ and
$w^{(i)} \in O$ for all $i$. We defined the distribution on hidden
states and words as follows in class:

\begin{equation*}
\Pr(W^{(1)},\dots,W^{(k)},C^{(1)},\cdots,C^{(k)})=\prod_{i=1}^k
\Pr(W^{(i)} \mid C^{(i)}=c^{(i)}, \vec{\theta}_{O, c^{(i)}})
\Pr(C^{(i)} \mid C^{(i-1)}=c^{(i-1)}, \vec{\theta}_{T, c^{(i-1)}})
\end{equation*}

Where $T$ are the state transitions, which here contain factors of $S$ of length 1, in other words $T$ is equivalent to $S$. Note that we are assuming (for the time being) that the parameters
$\vec{\theta}_{T, c^{(i-1)}}$ and $\vec{\theta}_{O, c^{(i)}}$ are
fixed, i.e. there is no uncertainty about their value. We will assume
throughout that there are three hidden states (categories) and three 
words. These are defined by:

\begin{lstlisting}
(def hidden-states '(Start N V))
(def vocabulary '(Call me Ishmael))
\end{lstlisting}

Note that, to make the exercises less painful, we are not including a
\texttt{stop} state. There are three state transition distributions
that we need to define, one for each of the hidden states:
$\Pr(C^{(i)} \mid C^{(i-1)}=\rtimes)$, $\Pr(C^{(i)} \mid C^{(i-1)}=N)$,
and $\Pr(C^{(i)} \mid C^{(i-1)}=V)$. As stated in the definition of
the HMM probabilities above, these distributions are specified by the
parameters $\vec{\theta}_{T, \rtimes}$, $\vec{\theta}_{T, N}$, and
$\vec{\theta}_{T, V}$:

\begin{lstlisting}
(def theta-transition-Start '(0 0.9 0.1))
(def theta-transition-N '(0 0.3 0.7))
(def theta-transition-V '(0 0.8 0.2))

(def theta-transition-dists-1 (list theta-transition-Start
                                    theta-transition-N
                                    theta-transition-V))
\end{lstlisting}

This means that the hidden state $N$ has probability $0.3$ of transitioning to
hidden state $N$, and probability $0.7$ of transitioning to $V$. Note that none
of the states ever transition to the $\rtimes$ hidden state.  Note also
that throughout this problem set we will assume that the initial hidden category
$C^{(0)}=\rtimes$ always.  This means that the $\rtimes$ state only occurs as
the first hidden state in a sequence, and never afterwards.

We also need to specify the observation distribution for the hidden
states $N$ and $V$ (we assume the hidden state \texttt{Start} does produce
observations). They are defined by:

\begin{lstlisting}
(def theta-observation-N '(0.1 0.5 0.4))
(def theta-observation-V '(0.8 0.1 0.1))

(def theta-observation-dists-1 (list theta-observation-N
                                     theta-observation-V))
\end{lstlisting}

This means, for example, that the hidden state $N$ has probability
$0.4$ of emitting the word $Ishmael$. The following helper functions
will be useful for writing your code.

\begin{lstlisting}
(defn dist-lookup [state states dists]
  (if (= state (first states))
      (first dists)
      (dist-lookup state (rest states) (rest dists))))

(defn log2 [n] (/ (Math/log n) (Math/log 2)))

(defn logsumexp [log-vals]
  (let [mx (apply max log-vals)]
    (+ mx
       (log2
           (apply +
                  (map (fn [z] (Math/pow 2 z))
                       (map (fn [x] (- x mx)) log-vals)))))))

(defn logscore-categorical [outcome outcomes params]
  (if (= outcome (first outcomes))
    (log2 (first params))
    (logscore-categorical outcome (rest outcomes) (rest params))))
\end{lstlisting}

\hrulefill%--------------------------------

\paragraph{Problem 1:}

Suppose that at time $t$ the hidden state is $c^{(t)}$. Then we can
compute the probability that the next hidden state is $c^{(t+1)}$ and
the next observed word is $w^{(t+1)}$. This probability is equal to:
\begin{align*}
&\Pr(W^{(t+1)} = w^{(t+1)}, C^{(t+1)} =c^{(t+1)} \mid C^{(t)} = c^{(t)}) \\ 
=  &\Pr(W^{(t+1)} = w^{(t+1)} \mid C^{(t+1)}=c^{(t+1)}) \Pr(C^{(t+1)} =c^{(t+1)} \mid C^{(t)} = c^{(t)})
\end{align*}

Write a procedure \required{\texttt{score-next-state-word}} which computes the 
log of this probability.
It should take five arguments: the current hidden state, the next
hidden state, and next observed word, the hidden state transition distributions,
and the observation distributions. A call to \texttt{(score-next-state-word
  current-hidden next-hidden next-observed theta-transition-dists
  theta-observation-dists)} should return the log probability of the next hidden
state and next observed word, given the current hidden state.

 Hint: use the functions \texttt{dist-lookup} and
\texttt{logscore-categorical} defined above.

\paragraph{Answer 1:} Please put the answer in
\texttt{ps\PSnum-lastname-firstname.clj}.

\hrulefill%--------------------------------

\paragraph{Problem 2:}

Suppose that at time $t$ the hidden state is $c^{(t)}$. Then we can
compute the \emph{marginal} probability of the word $w^{(t+1)}$ at
time $t+1$ given the hidden state at time $t$. This probability is
equal to:
\begin{align*}
&\Pr(W^{(t+1)} = w^{(t+1)} \mid C^{(t)} = c^{(t)}) \\ 
=&\sum_{c^{(t+1)}} \Pr(W^{(t+1)} 
= w^{(t+1)}, C^{(t+1)} =c^{(t+1)} \mid C^{(t)} = c^{(t)})\\
=&\sum_{c^{(t+1)}} \Pr(W^{(t+1)} 
= w^{(t+1)} \mid C^{(t+1)}=c^{(t+1)}) \Pr(C^{(t+1)} =c^{(t+1)} \mid C^{(t)} = c^{(t)})
\end{align*}

That is, we sum over all of the possible hidden states that could
appear at time $t+1$.

Using the procedure \texttt{score-next-state-word} that you wrote in
Problem~1, write a function
\required{\texttt{compute-next-observation-marginal}}, which takes four
arguments: \texttt{current-state}, \texttt{next-observation},
\texttt{theta-transition-dists}, and
\texttt{theta-observation-dists}. When
\texttt{(compute-next-observation-marginal current-state
  next-observation theta-transition-dists theta-observation-dists)} is
called, it should return the marginal probability of the next observed
word given the current hidden state.

Hint: use \texttt{logsumexp}. 

\paragraph{Answer 2:} Please put the answer in
\texttt{ps\PSnum-lastname-firstname.clj}.

\hrulefill%--------------------------------

\paragraph{Problem 3:}
Suppose that at time $t$ the hidden state is $c^{(t)}$. Then we
can compute the probability of the next $k$ hidden states and next
$k$ observed words given the hidden state at time $t$. This
probability is equal to:
\begin{align*}
&\Pr(W^{(t+1)},\dots,W^{(t+k)},C^{(t+1)},\cdots,C^{(t+k)} \mid C^{(t)}=c^{(t)} )\\
=&\prod_{i=t}^{t+k-1} \Pr(W^{(i+1)}=w^{(i+1)} \mid C^{(i+1)}=c^{(i+1)}) \Pr(C^{(i+1)} =c^{(i+1)}\mid C^{(i)}=c^{(i)}) \\
=&\Pr(C^{(t+1)} =c^{(t+1)}\mid C^{(t)}=c^{(t)}) \Pr(W^{(t+1)} =w^{(t+1)}\mid C^{(t+1)}=c^{(t+1)}) \\
&\cdot \Pr(W^{(t+2)},\dots,W^{(t+k)},C^{(t+2)},\cdots,C^{(t+k)} \mid C^{(t+1)}=c^{(t+1)} )
\end{align*}

Note that the equation shows that there is a recursive formula for
this probability.

Write a procedure \required{\texttt{score-next-states-words}} which computes this
probability. The function should take five arguments: the current
hidden state, a list of $k$ hidden states, a list of $k$ observed
words, the transition distributions, and the observation
distributions. When \texttt{(score-next-states-words current-hidden
  next-hidden-states next-words theta-transition-dists
  theta-observation-dists)} is called, it should return the log
probability that the $k$ hidden states and $k$ observed words will
appear immediately following the current hidden state.

You should use recursion to write the function. 

\paragraph{Answer 3:} Please put the answer in
\texttt{ps\PSnum-lastname-firstname.clj}.

\hrulefill%--------------------------------

\paragraph{Problem 4:}

Suppose the at time $t$ the hidden state is $c^{(t)}$. Then we can
compute the marginal probability of the next $k$ observed words
given this hidden state, i.e. summing out all of the possible settings
of the hidden states. This probability is equal to:
\begin{align*}
&\Pr(W^{(t+1)},\dots,W^{(t+k)} \mid C^{(t)}=c^{(t)} )\\ 
=& \sum_{c^{(t+1)},\cdots c^{(t+k)}} 
 \Pr(W^{(t+1)},\dots,W^{(t+k)},C^{(t+1)},\cdots,C^{(t+k)} \mid C^{(t)}=c^{(t)} ) \\
=& \sum_{c^{(t+1)},\cdots c^{(t+k)}} 
 \prod_{i=t}^{t+k-1} \Pr(W^{(i+1)}=w^{(i+1)} \mid C^{(i+1)}=c^{(i+1)}) \Pr(C^{(i+1)} =c^{(i+1)}\mid C^{(i)}=c^{(i)}) \\ 
= & \sum_{c^{(t+1)}}\left[ 
  \Pr(C^{(t+1)} =c^{(t+1)}\mid C^{(t)}=c^{(t)}) \Pr(W^{(t+1)} =w^{(t+1)}\mid C^{(t+1)}=c^{(t+1)})\right. \\
 &\phantom{\sum_{c^{(t+1)}}\ }\left.\cdot \Pr(W^{(t+2)},\dots,W^{(t+k)}) \mid C^{(t+1)}=c^{(t+1)} )\right]
\end{align*}

The final equation is very important. It shows that the marginal
probability of the words following hidden state $c^{(t)}$ can be
computed in a recursive manner. Write a function
\required{\texttt{compute-next-words-marginal}}, which computes the log marginal
probability of a sequence of observed words given the current hidden
state. The function should take four arguments: the current hidden
state, a list of words, the transition distributions, and the
observation distributions. When \texttt{(compute-next-words-marginal
  current-hidden next-words theta-transition-dists
  theta-observation-dists)} is called, it should return the log of the
marginal probability of the list of words given the hidden state, as
defined in the equation above.

You should use recursion to write your function. 

\paragraph{Answer 4:} Please put the answer in
\texttt{ps\PSnum-lastname-firstname.clj}.

\hrulefill%--------------------------------

\paragraph{Problem 5:}
Find a sequence of two words $w^{(1)},w^{(2)}$ such that the marginal
probability of this sequence is different than the marginal
probability of its reverse. That is, find $w^{(1)},w^{(2)}$ such that:
\begin{equation*}
  \Pr(W^{(1)}=w^{(1)}, W^{(2)} =w^{(2)} \mid C^{(0)}=\rtimes )
  \neq \Pr(W^{(1)}=w^{(2)}, W^{(2)} =w^{(1)} \mid C^{(0)}=\rtimes )
\end{equation*}

Use your solution to Problem 4 to confirm that your sequences have
different probabilities, and explain why this is true.

\paragraph{Answer 5:} Please put your answer in the box below.

\begin{AnswerBox}%% Do not delete %%%%%%%

    %%%%%%%%%%%%%%%%%%%%%%%
    %% YOUR ANSWER HERE. %%
    %%%%%%%%%%%%%%%%%%%%%%%
    Sequence 1: '(Call me)  probability: -4.2270

    Sequence 2: '(me Call)  probability: -1.9002

    The difference in these probabilities results from the fact that the HMM takes into account the order in which states and observations occur. The transition probabilities from the start to the first state that generates "call" or "me" and the subsequent transition to the state that generates the second word are different (see definition of transition and observation probabilities). Therefore, the probability that the sequences "Call me" and "me Call" are generated is different.

\end{AnswerBox}%% Do not delete %%%%%%%

\hrulefill%--------------------------------

\paragraph{Problem 6:}

In the next several problems, we will be using your solution to
Problem 4 to perform Bayesian inference in the HMM. Our goal will be
to compute the posterior distribution over hidden states given a
sequence of words. Recall that Bayes' Rule tells us how to calculate
this posterior distribution:
\begin{align*}
&\Pr(C^{(1)}=c^{(1)},\cdots,C^{(t)}=c^{(t)} \mid W^{(1)}=w^{(1)},\dots,W^{(t)}=w^{(t)} )\\
=&\frac{\Pr(C^{(1)}=c^{(1)},\cdots,C^{(t)}=c^{(t)})
        \Pr( W^{(1)}=w^{(1)},\dots,W^{(t)}=w^{(t)} 
        \mid C^{(1)}=c^{(1)},\cdots,C^{(t)}=c^{(t)})}
       {\Pr(W^{(1)}=w^{(1)},\dots,W^{(t)}=w^{(t)})}
\end{align*}

Write a procedure \required{\texttt{compute-hidden-prior}}, which takes two
arguments: a list of $k$ hidden states and the hidden state transition
distributions. When \texttt{(compute-hidden-prior hidden-states
  theta-transition-dists)} is called, it should return the log prior
probability of the hidden state sequence, i.e. the log value of
$$\Pr(C^{(1)}=c^{(1)},\cdots,C^{(k)}=c^{(k)})$$.

\paragraph{Answer 6:} Please put the answer in
\texttt{ps\PSnum-lastname-firstname.clj}.

\hrulefill%--------------------------------

\paragraph{Problem 7:}

In this problem, we will continue with the Bayesian calculation from
Problem 6. Write a procedure \required{\texttt{compute-likelihood-of-words}},
which takes three arguments: a list of $k$ hidden states, a list of
$k$ words, and the observation distributions. When
\texttt{(compute-likelihood-of-words hidden-states words
  theta-observation-dists)} is called, it should return the log
probability of the $k$ words being generated from the hidden states,
i.e. the log value of
$\Pr( W^{(1)}=w^{(1)},\dots,W^{(k)}=w^{(k)} \mid
C^{(1)}=c^{(1)},\cdots,C^{(k)}=c^{(k)})$.

\paragraph{Answer 7:} Please put the answer in
\texttt{ps\PSnum-lastname-firstname.clj}.

\hrulefill%--------------------------------

\paragraph{Problem 8:}
We are finally ready to solve our Bayesian inference problem. Write a
procedure \required{\texttt{compute-hidden-posterior}}, which takes four
arguments: a list of $k$ hidden states, a list of $k$ words, the
hidden state transition distributions, and the observation
distributions. When \texttt{(compute-hidden-posterior hidden-states
  words theta-transition-dists theta-observation-dists)} is called, it
should return the log posterior probability of the $k$ hidden states
given the $k$ words, i.e. it should return the log value of
$\Pr(C^{(1)}=c^{(1)},\cdots,C^{(k)}=c^{(k)} \mid%
W^{(1)}=w^{(1)},\dots,W^{(k)}=w^{(k)} )$.

Hint: Use your solutions to Problems 4, 6, and 7, and don't foget that
we are in the log domain. Note also that in this
function you will need to hardcode the first state as \texttt{'Start}.

\paragraph{Answer 8:} Please put the answer in
\texttt{ps\PSnum-lastname-firstname.clj}.

\hrulefill%--------------------------------

\paragraph{Problem 9:}
Suppose that we use the function \texttt{compute-next-words-marginal}
that you wrote in Problem 4, in order to compute the marginal
probability of a sequence of $k$ observed words. How many times is
\texttt{compute-next-words-marginal} called during the execution of the
program? What does this mean about the run-time of the program?

 Hint: your formula should involve the number of hidden
states and the length $k$ of the sequence of words.

\paragraph{Answer 9:} Please put your answer in the box below.

\begin{AnswerBox}%% Do not delete %%%%%%%

    %%%%%%%%%%%%%%%%%%%%%%%
    %% YOUR ANSWER HERE. %%
    %%%%%%%%%%%%%%%%%%%%%%%
    The total number of calls results from the tree of recursive calls, in which each node branches to $n$ new calls, where $n$ represents the number of hidden states. The depth of this tree is $k$ (the number of words). The total number of calls is therefore
    \begin{equation*}
        \sum_{i=0}^{k} n^i
    \end{equation*}
    The runtime of the program is therefore exponential in the number of words, i.e:
    $O(n^k)$

\end{AnswerBox}%% Do not delete %%%%%%%

\hrulefill%--------------------------------

\paragraph{Problem 10:}
In class, we introduced \emph{memoization} as a method for sharing
computation and/or reducing the run-time of programs. Memoization
refers to a technique where we intercept calls to a function with some
arguments, check if the function has been called with those arguments
before and, if not, call the function on those arguments, get the
result, store it in a table and return it. On subsequent calls to that
function with those arguments, we simply look up the result in the
table and avoid recomputing the value. In Clojure, functions can be
memoized by a call to \texttt{memoize}.

Using \texttt{memoize}, define a memoized version of your function
\texttt{compute-next-words-marginal}. Name this function
\required{\texttt{compute-next-words-marginal-mem}}.

\paragraph{Answer 10:} Please put the answer in
\texttt{ps\PSnum-lastname-firstname.clj}.

\hrulefill%--------------------------------

\paragraph{Problem 11:}
Suppose that we use the memoized function
\texttt{compute-next-words-marginal-mem} that you wrote in the
previous problem, in order to compute the marginal probability of a
sequence of $k$ observed words. How many times is
\texttt{compute-next-words-marginal-mem} called during the execution
of the program? How does this differ from the non-memoized version of
the procedure?

\paragraph{Answer 11:} Please put your answer in the box below.

\begin{AnswerBox}%% Do not delete %%%%%%%

    %%%%%%%%%%%%%%%%%%%%%%%
    %% YOUR ANSWER HERE. %%
    %%%%%%%%%%%%%%%%%%%%%%%
    In contrast to the non-memorized version, the total number of calls in the memoized version is significantly lower and is polynomial in k. This is because the compute-next-words-marginal-mem function is only called once for each unique combination of current state and remaining word sequence. In the non-memorized version, on the other hand, the result is recalculated for each recursive call, even if the same subsequence and the same state have already been calculated before.
    \begin{equation}
      Total Number of Calls = n \times \frac{k(k + 1)}{2}
    \end{equation}
    Runtime Complexity $ = O(nk^2)$

\end{AnswerBox}%% Do not delete %%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%

%--------------------------------
\newpage
\section*{LONG FORM READING QUESTION:} 
\textbf{(This section is optional for students in LING/COMP 445, but must be completed if taking LING 645.)}

You must answer this question on your own.

Kim et al.(2019) combines the traditional and neural network modelling approaches we have discussed in class. On the one hand their models make clear assumptions about the structure of language by defining neural network models that can only learn probabilistic context-free grammars, and on the other hand their models use distributed representations for both the observed and hidden variables, allowing models the freedom to encode additional information about the relations that may exist between these variables. This type of approach is sometimes called neuro-symbolic modelling. Based on all of the readings and discussions we have done and had this semester, in your opinion, (1) how might the traditional approach be better than the neural network approach? (2) how might the neural network approach be better than the traditional approach? and (3) how might a combined approach like Kim et al. be beneficial? There is no right or wrong answer, just make sure to properly motivate your position for each of these questions.

\paragraph{Answer:} Please put your answer in the box below.

\begin{AnswerBox}%% Do not delete %%%%%%%

  %%%%%%%%%%%%%%%%%%%%%%%
  %% YOUR ANSWER HERE. %%
  %%%%%%%%%%%%%%%%%%%%%%%

\end{AnswerBox}%% Do not delete %%%%%%%


\end{document}
